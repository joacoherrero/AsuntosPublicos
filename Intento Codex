# -*- coding: utf-8 -*-
"""
Created on Wed Jun 11 11:07:58 2025

@author: herre
"""
import feedparser
import os
from openai import OpenAI
import logging
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
from dateutil import parser
from collections import defaultdict
import re

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('boletin_processor.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def scrape_senado():
    # URL de la p√°gina
    url = "https://www.senado.gob.ar/parlamentario/comisiones/?active=permanente"

    try:
        print("Accediendo a la URL:", url)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        print("Procesando contenido...")
        soup = BeautifulSoup(response.text, 'html.parser')

        # Lista para almacenar todas las reuniones
        reuniones = []

        # Encontrar la tabla de agenda de reuniones
        # La tabla est√° despu√©s del encabezado "Agenda de Reuniones"
        agenda_header = soup.find('h1', string='Agenda de Reuniones')
        if not agenda_header:
            print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de agenda")
            return None

        # Buscar la tabla despu√©s del encabezado
        tabla = agenda_header.find_next('table')
        if not tabla:
            print("‚ö†Ô∏è No se encontr√≥ la tabla de reuniones")
            return None

        # Procesar las filas de la tabla
        rows = tabla.find_all('tr')
        print(f"Se encontraron {len(rows)} filas en la tabla")

        for row in rows:
            # Obtener todas las celdas de la fila
            cells = row.find_all('td')

            if len(cells) == 3:  # La tabla tiene 3 columnas: Comisi√≥n, D√≠a y Hora, Pr√≥xima Reuni√≥n
                comision = cells[0].get_text(strip=True)
                dia_hora = cells[1].get_text(strip=True)
                agenda_link = cells[2].find('a')['href'] if cells[2].find('a') else ""

                # Procesar d√≠a y hora
                # El formato suele ser "ASESORES - D√≠a de la semana DD de mes - HH:mm h"
                dia_hora_parts = dia_hora.split('-')
                tipo_reunion = dia_hora_parts[0].strip() if len(dia_hora_parts) > 1 else ""

                # Extraer fecha y hora usando expresiones regulares
                fecha_match = re.search(r'(\w+\s+\d+\s+de\s+\w+)', dia_hora)
                hora_match = re.search(r'(\d{1,2}:\d{2})\s*h', dia_hora)

                fecha = fecha_match.group(1) if fecha_match else ""
                hora = hora_match.group(1) if hora_match else ""

                reunion = {
                    'comision': comision,
                    'tipo_reunion': tipo_reunion,
                    'fecha': fecha,
                    'hora': hora,
                    'dia_hora_completo': dia_hora,
                    'agenda_url': f"https://www.senado.gob.ar{agenda_link}" if agenda_link else ""
                }

                reuniones.append(reunion)
                print(f"\n‚úì Nueva reuni√≥n encontrada:")
                print(f"  Comisi√≥n: {comision}")
                print(f"  Tipo: {tipo_reunion}")
                print(f"  Fecha: {fecha}")
                print(f"  Hora: {hora}")

        print(f"\nTotal de reuniones encontradas: {len(reuniones)}")

        if not reuniones:
            print("‚ö†Ô∏è No se encontraron reuniones")
            return None

        # Crear DataFrame
        df = pd.DataFrame(reuniones)

        # Guardar en CSV
        filename = f'agenda_senado_{datetime.now().strftime("%Y%m%d_%H%M")}.csv'
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\n‚úÖ Datos guardados exitosamente en {filename}")

        # Mostrar las primeras filas del DataFrame
        print("\nPrimeras entradas del DataFrame:")
        print(df.head())

        return df

    except Exception as e:
        print(f"‚ùå Error durante el procesamiento: {e}")
        import traceback
        print(traceback.format_exc())
        return None


def scrape_comisiones():
    # URL de la p√°gina
    url = "https://www.hcdn.gov.ar/comisiones/agenda/"

    try:
        print("Accediendo a la URL:", url)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()

        print("Procesando contenido...")
        soup = BeautifulSoup(response.text, 'html.parser')

        # Lista para almacenar todas las reuniones
        reuniones = []
        fecha_actual = None

        # Encontrar la tabla principal
        tabla = soup.find('table')
        if not tabla:
            print("‚ö†Ô∏è No se encontr√≥ ninguna tabla en la p√°gina")
            return None

        # Encontrar todas las filas de la tabla
        rows = tabla.find_all('tr')
        print(f"Se encontraron {len(rows)} filas en la tabla")

        # Debug: Mostrar el HTML de las filas que podr√≠an contener fechas
        print("\nExaminando filas para fechas:")
        for row in rows:
            if len(row.find_all('td')) != 2:  # Si no tiene dos celdas, podr√≠a ser una fecha
                print("\nPosible fila de fecha:")
                print(row.prettify())

        # Procesar las filas
        for row in rows:
            # Primero intentamos encontrar si es una fila de fecha (th)
            th = row.find('th')
            if th and th.get('colspan') == '2':
                texto_fecha = th.get_text(strip=True)
                if any(dia in texto_fecha.lower() for dia in ['lunes', 'martes', 'mi√©rcoles', 'jueves', 'viernes']):
                    fecha_actual = texto_fecha
                    print(f"\nüìÖ Nueva fecha encontrada: {fecha_actual}")
                    continue

            # Si no es fecha, procesamos como reuni√≥n si tiene dos celdas
            cells = row.find_all('td')
            if len(cells) == 2:
                texto_celda1 = cells[0].get_text(strip=True)
                texto_celda2 = cells[1].get_text(strip=True)

                # Extraer hora y sala de la primera celda
                hora_match = re.match(r'(\d{1,2}:\d{2})(.*)', texto_celda1)
                if hora_match:
                    hora = hora_match.group(1)
                    sala = hora_match.group(2).strip()

                    # Extraer comisi√≥n y descripci√≥n de la segunda celda
                    partes = texto_celda2.split('.')
                    comision = partes[0].strip()
                    descripcion = '.'.join(partes[1:]).strip() if len(partes) > 1 else ''

                    # Buscar el enlace de la citaci√≥n
                    citacion_link = cells[1].find('a', href=True)
                    citacion_url = citacion_link['href'] if citacion_link else ""

                    reunion = {
                        'fecha': fecha_actual,
                        'hora': hora,
                        'sala': sala,
                        'comision': comision,
                        'descripcion': descripcion,
                        'citacion_url': citacion_url
                    }

                    reuniones.append(reunion)
                    print(f"\n‚úì Nueva reuni√≥n encontrada:")
                    print(f"  Fecha: {fecha_actual}")
                    print(f"  Hora: {hora}")
                    print(f"  Sala: {sala}")
                    print(f"  Comisi√≥n: {comision}")
                    print(f"  Descripci√≥n: {descripcion[:100]}...")

        print(f"\nTotal de reuniones encontradas: {len(reuniones)}")

        if not reuniones:
            print("‚ö†Ô∏è No se encontraron reuniones")
            return None

        # Crear DataFrame
        df = pd.DataFrame(reuniones)

        # Guardar en CSV
        filename = f'agenda_comisiones_{datetime.now().strftime("%Y%m%d_%H%M")}.csv'
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\n‚úÖ Datos guardados exitosamente en {filename}")

        # Mostrar las primeras filas del DataFrame
        print("\nPrimeras entradas del DataFrame:")
        print(df.head())

        return df

    except Exception as e:
        print(f"‚ùå Error durante el procesamiento: {e}")
        import traceback
        print(traceback.format_exc())
        return None


class BoletinDownloader:
    """Clase para descargar el Bolet√≠n Oficial"""

    def __init__(self):
        self.base_url = "https://www.boletinoficial.gob.ar"
        self.pdf_url_template = "https://www.boletinoficial.gob.ar/pdf/pdfPorNombre/{fecha}"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def obtener_ultimo_boletin(self) -> tuple[str, str]:
        """Obtiene la URL del √∫ltimo Bolet√≠n Oficial publicado"""
        try:
            logger.info("Obteniendo URL del √∫ltimo Bolet√≠n Oficial...")

            # Obtener la fecha actual
            fecha_actual = datetime.now()

            # Si es fin de semana, retroceder al viernes
            while fecha_actual.weekday() > 4:  # 5 = S√°bado, 6 = Domingo
                fecha_actual = fecha_actual - timedelta(days=1)

            # Formatear la fecha para la URL
            fecha_str = fecha_actual.strftime("%Y%m%d")

            # Construir la URL del PDF
            pdf_url = self.pdf_url_template.format(fecha=fecha_str)
            nombre_archivo = f"boletin_{fecha_str}.pdf"

            # Verificar si el PDF existe
            response = requests.head(pdf_url, headers=self.headers)
            if response.status_code == 404:
                # Si no existe, probar con el d√≠a anterior
                fecha_actual = fecha_actual - timedelta(days=1)
                fecha_str = fecha_actual.strftime("%Y%m%d")
                pdf_url = self.pdf_url_template.format(fecha=fecha_str)
                nombre_archivo = f"boletin_{fecha_str}.pdf"

                # Verificar nuevamente
                response = requests.head(pdf_url, headers=self.headers)
                if response.status_code == 404:
                    raise ValueError(f"No se encontr√≥ el Bolet√≠n para la fecha {fecha_str}")

            logger.info(f"URL del Bolet√≠n encontrada: {pdf_url}")
            return pdf_url, nombre_archivo

        except Exception as e:
            logger.error(f"Error al obtener la URL del Bolet√≠n: {str(e)}")
            raise


def cargar_diccionario(path='Diccionario.xlsx'):
    """Carga los temas y palabras clave desde un archivo Excel."""
    if not os.path.exists(path):
        print(f"Diccionario no encontrado: {path}")
        return {}

    df = pd.read_excel(path, engine='openpyxl')
    temas = {}
    for _, row in df.iterrows():
        if pd.notna(row.iloc[0]):
            tema = str(row.iloc[0]).strip()
            temas.setdefault(tema, {'tema': tema, 'palabras_clave': []})
            if len(row) > 1 and pd.notna(row.iloc[1]):
                palabra = str(row.iloc[1]).strip().lower()
                temas[tema]['palabras_clave'].append(palabra)
    return temas


def cargar_cuentas(path='Cuentas.xlsx'):
    """Carga informaci√≥n de cuentas y sus temas asociados."""
    if not os.path.exists(path):
        print(f"Cuentas no encontradas: {path}")
        return {}

    df = pd.read_excel(path, engine='openpyxl')
    cuentas = {}
    for _, row in df.iterrows():
        empresa = row.get('Empresa')
        if pd.isna(empresa):
            continue
        empresa = str(empresa).strip()
        cuentas.setdefault(empresa, {'nombre': empresa, 'temas': []})
        for col in df.columns:
            if col != 'Empresa' and pd.notna(row[col]):
                tema = str(row[col]).strip()
                if tema and tema not in cuentas[empresa]['temas']:
                    cuentas[empresa]['temas'].append(tema)
    return cuentas


def es_de_hoy(date_str):
    if not date_str:
        return False
    try:
        published = parser.parse(date_str)
        now = datetime.now()
        return published.date() == now.date()
    except Exception:
        return False


def obtener_noticias(feeds):
    noticias = []
    for nombre, url in feeds.items():
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                fecha = entry.published if 'published' in entry else entry.get('updated')
                if es_de_hoy(fecha):
                    noticias.append({
                        'title': entry.title,
                        'link': entry.link,
                        'published': fecha,
                        'feed_name': nombre
                    })
        except Exception as e:
            print(f"Error procesando feed {nombre}: {e}")
    return noticias


def clasificar_noticias(noticias, temas):
    noticias_por_tema = defaultdict(list)
    for noticia in noticias:
        titulo = noticia['title'].lower()
        for tema, data in temas.items():
            for palabra in data['palabras_clave']:
                if palabra in titulo:
                    noticia['palabra_encontrada'] = palabra
                    noticias_por_tema[tema].append(noticia)
                    break
    return noticias_por_tema


import os
from openai import OpenAI

def resumen_noticias(noticias):
    client = OpenAI(api_key="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")
    
    texto = "\n".join(n['title'] for n in noticias[:20])
    prompt = f"Resum√≠ en detalle usando items, y juntando toda la informaci√≥n de cada t√≥pico, el contexto pol√≠tico a partir de estos titulares:\n{texto}"

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=150,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Error al generar resumen: {e}"



def obtener_agenda_senado():
    df = scrape_senado()
    if df is None:
        return pd.DataFrame()
    hoy = datetime.now().strftime('%d')
    return df[df['fecha'].str.contains(hoy, na=False)]


def obtener_agenda_diputados():
    df = scrape_comisiones()
    if df is None:
        return pd.DataFrame()
    hoy = datetime.now().strftime('%d')
    return df[df['fecha'].str.contains(hoy, na=False)]


def obtener_boletin():
    try:
        downloader = BoletinDownloader()
        url, nombre = downloader.obtener_ultimo_boletin()
        return url
    except Exception as e:
        return f"Error obteniendo bolet√≠n: {e}"


def generar_reporte():
    temas = cargar_diccionario()
    cuentas = cargar_cuentas()

    feeds = {
            'infobae': 'https://www.infobae.com/feeds/rss/',
            'clarin': 'https://www.clarin.com/rss/lo-ultimo/',
            'lanacion_principal': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/?outputType=xml',
            'lanacion_politica': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/categoria/politica/?outputType=xml',
            'lanacion_economia': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/categoria/economia/?outputType=xml',
            'lanacion_deportes': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/categoria/deportes/?outputType=xml',
            'lanacion_sociedad': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/categoria/sociedad/?outputType=xml',
            'lanacion_mundo': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/categoria/el-mundo/?outputType=xml',
            'lanacion_tecnologia': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/categoria/tecnologia/?outputType=xml',
            'lanacion_opinion': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/categoria/opinion/?outputType=xml',
            'lanacion_lifestyle': 'https://www.lanacion.com.ar/arc/outboundfeeds/rss/categoria/lifestyle/?outputType=xml',
            # La Pol√≠tica Online
            'lpo_ultimas': 'http://www.lapoliticaonline.com.ar/files/rss/ultimasnoticias.xml',
            'lpo_politica': 'http://www.lapoliticaonline.com.ar/files/rss/politica.xml',
            'lpo_economia': 'http://www.lapoliticaonline.com.ar/files/rss/economia.xml',
            # Feeds de medios regionales
            'misiones_online': 'https://misionesonline.net/feed/',
            'eldia': 'https://www.eldia.com/.rss',
            'rionegro': 'https://www.rionegro.com.ar/feed/',
            'diario_cuyo': 'https://www.diariodecuyo.com.ar/rss/rss.xml',
            # Feeds de La Gaceta
            'lagaceta_general': 'https://feeds.feedburner.com/LaGaceta-General',
            'lagaceta_politica': 'https://www.lagaceta.com.ar/rss/politica.xml',
            'lagaceta_economia': 'https://www.lagaceta.com.ar/rss/economia.xml',
            'lagaceta_deportes': 'https://www.lagaceta.com.ar/rss/deportes.xml',
            'lagaceta_sociedad': 'https://www.lagaceta.com.ar/rss/sociedad.xml',
            'lagaceta_mundo': 'https://www.lagaceta.com.ar/rss/mundo.xml',
            'lagaceta_espectaculos': 'https://www.lagaceta.com.ar/rss/espectaculos.xml',
            'lagaceta_opinion': 'https://www.lagaceta.com.ar/rss/opinion.xml',
            'pagina12': 'https://www.pagina12.com.ar/rss/portada',
            'cronista': 'https://www.cronista.com/files/rss/news.xml',
            'tn': 'https://tn.com.ar/rss',
            'mdz_politica': 'https://www.mdzol.com/rss/feed.html?r=1',
            'mdz_opinion': 'https://www.mdzol.com/rss/feed.html?r=4',
            'mdz_dinero': 'https://www.mdzol.com/rss/feed.html?r=5',
            'mdz_mundo': 'https://www.mdzol.com/rss/feed.html?r=6',
            'mdz_sociedad': 'https://www.mdzol.com/rss/feed.html?r=7',
            'mdz_deportes': 'https://www.mdzol.com/rss/feed.html?r=8',
            'mdz_sociales': 'https://www.mdzol.com/rss/feed.html?r=9',
            'mdz_show': 'https://www.mdzol.com/rss/feed.html?r=10',
            'mdz_espectaculos': 'https://www.mdzol.com/rss/feed.html?r=82',
            'mdz_policiales': 'https://www.mdzol.com/rss/feed.html?r=84',
            'mdz_energia': 'https://www.mdzol.com/rss/feed.html?r=90',
            # Feeds de √Åmbito Financiero
            'ambito_home': 'https://www.ambito.com/rss/pages/home.xml',
            'ambito_economia': 'https://www.ambito.com/rss/pages/economia.xml',
            'ambito_ultimas': 'https://www.ambito.com/rss/pages/ultimas-noticias.xml',
            'ambito_politica': 'https://www.ambito.com/rss/pages/politica.xml',
            'ambito_tecnologia': 'https://www.ambito.com/rss/pages/tecnologia.xml',
            # Feeds de La Voz
            'lavoz_principal': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?origen=1',
            'lavoz_noticias': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?origen=2',
            'lavoz_politica': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?categoria=1',
            'lavoz_negocios': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?categoria=48',
            'lavoz_ciudadanos': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?categoria=17',
            'lavoz_deportes': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?categoria=18',
            'lavoz_cultura': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?categoria=16',
            'lavoz_internacionales': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?categoria=47',
            'lavoz_espectaculos': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?categoria=213',
            'lavoz_opinion': 'http://archivo.lavoz.com.ar/RSS/RSS.asp?categoria=214',
        }

    noticias = obtener_noticias(feeds)
    noticias_por_tema = clasificar_noticias(noticias, temas)

    resumen = resumen_noticias(noticias)
    agenda_senado = obtener_agenda_senado()
    agenda_diputados = obtener_agenda_diputados()
    boletin = obtener_boletin()

    from docx import Document
    doc = Document()
    doc.add_heading('Informe Diario', 0)
    doc.add_paragraph('Esquema del documento:')
    doc.add_paragraph('1. Sesiones del d√≠a')
    doc.add_paragraph('2. Resumen de noticias')
    doc.add_paragraph('3. Noticias por tema')

    doc.add_heading('Sesiones del d√≠a', level=1)
    if not agenda_diputados.empty:
        doc.add_heading('Diputados', level=2)
        for _, row in agenda_diputados.iterrows():
            doc.add_paragraph(f"{row['fecha']} {row['hora']} - {row['comision']}")
    if not agenda_senado.empty:
        doc.add_heading('Senado', level=2)
        for _, row in agenda_senado.iterrows():
            doc.add_paragraph(f"{row['fecha']} {row['hora']} - {row['comision']}")

    if boletin:
        doc.add_paragraph(f"Bolet√≠n Oficial: {boletin}")

    doc.add_heading('Resumen de contexto pol√≠tico', level=1)
    doc.add_paragraph(resumen)

    doc.add_heading('Noticias por tema', level=1)
    for tema, items in noticias_por_tema.items():
        doc.add_heading(f"{tema} ({len(items)})", level=2)
        for n in items:
            doc.add_paragraph(f"- {n['title']} ({n['feed_name']})")

    nombre = f"informe_{datetime.now().strftime('%Y%m%d')}.docx"
    doc.save(nombre)
    print(f"Reporte generado: {nombre}")


if __name__ == '__main__':
    generar_reporte()
